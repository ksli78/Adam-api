# Mistral Small 22B Setup Guide

This guide explains how to set up **Mistral Small 22B** - the recommended model for your 32GB VRAM setup.

## Why Mistral Small 22B?

**Mistral AI** is a French AI company focused on open, efficient models. Mistral Small is specifically designed for:
- ‚úÖ **RAG applications** (Retrieval Augmented Generation)
- ‚úÖ **Document understanding** and semantic matching
- ‚úÖ **Instruction following** for complex tasks
- ‚úÖ **128K context window** (same as llama3.1)
- ‚úÖ **Commercial use** allowed

### Advantages Over Other Models

| Feature | Mistral Small 22B | llama3.1:8b | llama3.1:70b | Qwen (Chinese) |
|---------|-------------------|-------------|--------------|----------------|
| **VRAM Usage** | 22 GB ‚úÖ | 8 GB | 40 GB ‚ùå | Varies |
| **Fits 32GB?** | Yes | Yes | No (overflow) | Yes |
| **Speed** | Medium (6-10s) | Fast (3-5s) | Very Slow (2+ min) | Medium |
| **Reasoning** | Excellent ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Good ‚≠ê‚≠ê‚≠ê | Best ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Good ‚≠ê‚≠ê‚≠ê‚≠ê |
| **RAG Quality** | Excellent ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Good ‚≠ê‚≠ê‚≠ê | Excellent ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Good ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Company** | üá´üá∑ France | üá∫üá∏ USA (Meta) | üá∫üá∏ USA (Meta) | üá®üá≥ China |
| **License** | Apache 2.0 | Llama 3.1 | Llama 3.1 | Varies |

**Best balance for 32GB VRAM!**

---

## Installation Steps

### Step 1: Pull Mistral Small on Remote Server

SSH into your Ollama server:

```bash
ssh user@adam.amentumspacemissions.com
```

Pull the model (~12 GB download):

```bash
docker exec ollama ollama pull mistral-small:22b
```

**Download time:** 5-15 minutes depending on your internet connection.

### Step 2: Pre-load and Test the Model

Pre-load the model with keep-alive:

```bash
# Load the model
docker exec ollama ollama run mistral-small:22b "What is 2+2?"

# Expected: Should respond "4" or "The answer is 4"
# Model will load into VRAM and stay there
```

Verify it's loaded:

```bash
docker exec ollama ollama ps
```

**Expected output:**
```
NAME                ID              SIZE      PROCESSOR    UNTIL
mistral-small:22b   abc123def456    22 GB     100% GPU     Forever
```

### Step 3: Monitor GPU Usage

Watch GPU memory during load:

```bash
watch -n 1 nvidia-smi
```

**Expected:**
```
+-----------------------------------------------------------------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
|   0  GPU 0           On       | ...          Off     |                    0 |
| 40%   45C    P2    180W / 250W |  11000MiB / 16384MiB |     95%      Default |
|   1  GPU 1           On       | ...          Off     |                    0 |
| 40%   45C    P2    180W / 250W |  11000MiB / 16384MiB |     95%      Default |
+-----------------------------------------------------------------------------+
```

**VRAM Distribution:**
- GPU 0: ~11 GB
- GPU 1: ~11 GB
- Total: ~22 GB (perfect fit!)

---

## Application Configuration

### Step 4: Pull Latest Code on Application Server

```bash
# Navigate to your Adam-api directory
cd /path/to/Adam-api

# Pull the updated code
git pull origin claude/fix-torch-compiler-error-011CUqEPVfgyByW8D6bLk7ob
```

The code is already configured to use Mistral Small by default:
- `LLM_MODEL = "mistral-small:22b"`
- `LLM_CONTEXT_WINDOW = 16384` (16K tokens, optimized for 32GB VRAM)
- `OLLAMA_HOST = "http://adam.amentumspacemissions.com:11434"`

### Step 5: Restart Application

```bash
# If using Docker Compose
docker-compose restart

# OR if running directly
python airgapped_rag_advanced.py
```

### Step 6: Verify Configuration

Check the startup logs for:

```
INFO - MetadataExtractor initialized: model=mistral-small:22b, host=http://adam.amentumspacemissions.com:11434, context_window=16384
```

---

## Testing

### Test 1: Document Selection Query

```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/query' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "prompt": "How many hours can I work in a single week?",
  "use_llm_selection": true,
  "max_documents": 10
}'
```

**Expected Results:**
- ‚è±Ô∏è Response time: **6-10 seconds**
- üìÑ Selects: **EN-PO-0301.pdf** (work hours policy)
- ‚úÖ Accurate answer about work hours from the policy

### Test 2: Complex Question

```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/query' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "prompt": "What are the requirements for submitting an expense reimbursement?",
  "use_llm_selection": true,
  "max_documents": 10
}'
```

Mistral Small should excel at understanding complex, multi-part questions!

### Test 3: Monitor Performance

While testing, watch the logs for:

```
INFO - Stage 1: Using hybrid search to find top 30 candidate documents
INFO - Stage 1 complete: Narrowed to 30 candidate documents
INFO - Stage 2: Using LLM to select best documents from candidates
INFO - Stage 2 complete: LLM selected 2 final documents:
INFO -   - EN-PO-0301.pdf (type: policy)
```

And GPU usage on remote server:

```bash
watch -n 1 nvidia-smi
```

Should show **~22GB VRAM usage** and **80-100% GPU utilization** during inference.

---

## Performance Expectations

### Loading Time
- **First load:** 15-30 seconds (loading model into VRAM)
- **With keep-alive:** Stays loaded, instant subsequent requests

### Inference Time
- **Document selection** (Stage 2): 5-8 seconds
- **Answer generation:** 6-10 seconds
- **Total query time:** 8-15 seconds

### GPU Utilization
- **VRAM:** ~22GB (11GB per GPU)
- **Utilization:** 80-100% during inference
- **Idle:** Model stays in VRAM, 0% utilization

---

## Comparison: Before vs After

### Previous Setup (llama3.1:70b)
- ‚ùå VRAM: 40GB (overflow to RAM)
- ‚ùå Speed: 2+ minutes per query
- ‚ùå Crashes: "exit status 2" errors
- ‚ùå Unstable

### Current Setup (Mistral Small 22B)
- ‚úÖ VRAM: 22GB (perfect fit)
- ‚úÖ Speed: 8-15 seconds per query (10x faster!)
- ‚úÖ Stable: No memory errors
- ‚úÖ Better reasoning than llama3.1:8b
- ‚úÖ European company (Mistral AI, France)

---

## Mistral Small Advantages for RAG

Mistral Small was specifically designed for tasks like yours:

1. **Document Understanding**
   - Excellent at semantic similarity matching
   - Better at understanding document relationships
   - Trained on instruction-following tasks

2. **Context Handling**
   - 128K context window (same as llama3.1)
   - Efficient use of context (low memory overhead)
   - Handles 30+ documents in Stage 2 easily

3. **Reasoning Quality**
   - Better than llama3.1:8b at complex questions
   - Nearly as good as llama3.1:70b for RAG tasks
   - Specifically tuned for retrieval tasks

4. **Instruction Following**
   - Excellent at following JSON output formats
   - Understands complex prompts for document selection
   - Reliable structured output

---

## Troubleshooting

### Issue: Model Takes Too Long to Load

**Symptom:** First query takes 30-60 seconds

**Solution:** This is normal for first load. Use keep-alive to keep it hot:
```bash
# Set OLLAMA_KEEP_ALIVE=-1 in Portainer
# Or pre-load after container restart
docker exec ollama ollama run mistral-small:22b ""
```

### Issue: "Out of Memory" Error

**Symptom:** Model fails to load or crashes

**Possible causes:**
1. Other processes using VRAM
2. Model not distributed across GPUs properly

**Solutions:**
```bash
# Check what's using VRAM
nvidia-smi

# Restart Ollama container
docker restart ollama

# Try loading again
docker exec ollama ollama run mistral-small:22b "test"
```

### Issue: Slower Than Expected

**Symptom:** Queries take 15-20 seconds instead of 8-15

**Possible causes:**
1. Model not fully in VRAM (check nvidia-smi)
2. CPU bottleneck
3. Network latency

**Solutions:**
```bash
# Verify GPU usage during inference
watch -n 1 nvidia-smi

# Check Ollama logs
docker logs ollama --tail 50

# Test direct Ollama performance
time docker exec ollama ollama run mistral-small:22b "test"
# Should complete in 3-5 seconds
```

### Issue: Wrong Documents Selected

**Symptom:** Mistral selects irrelevant documents

**Possible causes:**
1. Stage 1 (hybrid search) not finding right candidates
2. Poor quality questions in metadata

**Solutions:**
1. Check Stage 1 candidates in logs - is the right document there?
2. Consider re-ingesting documents with Mistral Small for better questions
3. Adjust Stage 1 parameters (BM25 weight, top_k)

---

## Advanced Configuration

### Context Window Tuning

The default 16K context window is optimized for 32GB VRAM stability:

```bash
# Current optimized setting (default)
export LLM_CONTEXT_WINDOW="16384"  # 16K tokens, ~20 docs in Stage 2

# If you have VRAM headroom and need more documents
export LLM_CONTEXT_WINDOW="32768"  # 32K tokens, ~30 docs in Stage 2

# If still experiencing crashes, reduce further
export LLM_CONTEXT_WINDOW="8192"   # 8K tokens, ~10 docs in Stage 2
```

**Important:**
- 16K (default) balances stability and capacity
- Higher values use more VRAM and may cause crashes
- Lower values are more stable but select from fewer candidates
- Restart application after changing: `docker-compose restart`

### Adjust Temperature

For more deterministic responses:

```python
# In airgapped_rag_advanced.py
# Document selection already uses temperature=0.1 (good!)

# For answer generation, you can adjust:
export TEMPERATURE="0.1"  # More deterministic
export TEMPERATURE="0.3"  # More creative (default)
```

### Use Different Quantization (If Available)

If you need to save VRAM:

```bash
# Q4 quantization (~15GB instead of 22GB)
docker exec ollama ollama pull mistral-small:22b-q4
export LLM_MODEL="mistral-small:22b-q4"
```

**Trade-off:** Slightly lower quality, but faster and uses less VRAM.

---

## Optional: Re-ingest Documents

Mistral Small generates **better quality questions** than llama3.1:8b during document ingestion.

If you want to improve question quality:

```bash
# 1. Clear existing documents
curl -X DELETE http://localhost:8000/documents

# 2. Re-upload your documents
# The metadata_extractor will now use Mistral Small to generate better questions

# 3. Test improved document selection
curl -X POST http://localhost:8000/query ...
```

**Benefits:**
- More accurate answerable_questions
- Better semantic matching
- Improved document selection accuracy

---

## Support & Resources

### Mistral AI Resources
- Website: https://mistral.ai
- Documentation: https://docs.mistral.ai
- Model card: https://huggingface.co/mistralai/Mistral-Small-22B

### Ollama Resources
- Mistral Small page: https://ollama.com/library/mistral-small
- Ollama docs: https://ollama.com/docs

### Monitoring
```bash
# GPU usage
nvidia-smi

# Ollama running models
docker exec ollama ollama ps

# Ollama logs
docker logs ollama -f

# Application logs
docker-compose logs -f
```

---

## Summary

**Mistral Small 22B** is the optimal choice for your 32GB VRAM setup:

‚úÖ **Perfect fit:** 22GB VRAM usage
‚úÖ **Fast:** 8-15 second queries
‚úÖ **High quality:** Excellent reasoning for RAG
‚úÖ **Stable:** No memory overflow
‚úÖ **European:** Mistral AI (France)
‚úÖ **Open:** Apache 2.0 license

Enjoy your much faster and more reliable RAG system! üöÄ
